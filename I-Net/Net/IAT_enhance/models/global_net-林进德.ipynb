{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe000f08-198a-463b-9643-5f56402bda92",
   "metadata": {},
   "source": [
    "# 模块导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30248a-61ea-491f-86a2-3053bb285d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import trunc_normal_, DropPath, to_2tuple\n",
    "import os\n",
    "from model.blocks import Mlp  # 复用前面定义好的 MLP 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69691a11-0653-408b-9219-5847f4c273c7",
   "metadata": {},
   "source": [
    "# 自定义 Query Attention 注意力模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b50da2-9fad-4f67-bb47-a6f99c1c9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class query_Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    全局模块专用自注意力机制，使用固定可训练的 Query 向量\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=2, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # 固定长度可训练 Query\n",
    "        self.q = nn.Parameter(torch.ones((1, 10, dim)), requires_grad=True)\n",
    "        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # 计算 K 和 V\n",
    "        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 生成广播型 Q 向量\n",
    "        q = self.q.expand(B, -1, -1).view(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 多头注意力计算\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, 10, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3613e-c10f-48a7-a003-e44aa3e9ccf4",
   "metadata": {},
   "source": [
    "# 全局模块的自注意力残差块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8c5b3-0111-4104-89b3-2701dba5034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class query_SABlock(nn.Module):\n",
    "    \"\"\"\n",
    "    全局模块：自注意力 + MLP 残差模块\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0.,\n",
    "                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)  # 位置卷积增强\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = query_Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                     attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7164be15-629c-410e-851a-2e21ba4cca00",
   "metadata": {},
   "source": [
    "# 卷积特征提取模块（下采样特征抽取）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e51c2-ad16-47d4-b948-7f7fe453e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    使用卷积提取初步全局特征（下采样两次）\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(conv_embedding, self).__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels // 2, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27be36e-1cb3-4936-a77c-d39a0d26c8de",
   "metadata": {},
   "source": [
    "# 完整的 Global Prediction 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6afe7-f36d-41ab-bfc3-bcb3f9425d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_pred(nn.Module):\n",
    "    \"\"\"\n",
    "    全局模块，预测颜色矩阵 + gamma 值，用于全局曝光色彩调整\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=64, num_heads=4, type='exp'):\n",
    "        super(Global_pred, self).__init__()\n",
    "\n",
    "        # γ (gamma) 初始化：曝光增强时固定为1\n",
    "        if type == 'exp':\n",
    "            self.gamma_base = nn.Parameter(torch.ones((1)), requires_grad=False)\n",
    "        else:\n",
    "            self.gamma_base = nn.Parameter(torch.ones((1)), requires_grad=True)\n",
    "\n",
    "        # 初始化基础颜色矩阵为单位矩阵\n",
    "        self.color_base = nn.Parameter(torch.eye(3), requires_grad=True)\n",
    "\n",
    "        # 卷积特征提取 + 注意力建模\n",
    "        self.conv_large = conv_embedding(in_channels, out_channels)\n",
    "        self.generator = query_SABlock(dim=out_channels, num_heads=num_heads)\n",
    "\n",
    "        # γ 和 color 输出层\n",
    "        self.gamma_linear = nn.Linear(out_channels, 1)\n",
    "        self.color_linear = nn.Linear(out_channels, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # 注意力权重初始化：抑制 V分支，训练更稳定\n",
    "        for name, p in self.named_parameters():\n",
    "            if name == 'generator.attn.v.weight':\n",
    "                nn.init.constant_(p, 0)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_large(x)  # 卷积提取特征\n",
    "        x = self.generator(x)   # 通过自注意力建模全局信息\n",
    "\n",
    "        gamma, color = x[:, 0].unsqueeze(1), x[:, 1:]\n",
    "        gamma = self.gamma_linear(gamma).squeeze(-1) + self.gamma_base\n",
    "        color = self.color_linear(color).squeeze(-1).view(-1, 3, 3) + self.color_base\n",
    "\n",
    "        return gamma, color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf10ca5-39f6-4e99-a1ad-000d85d41ee7",
   "metadata": {},
   "source": [
    "# 简单测试模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f10e3-dc97-4331-9ca7-83b5f6a591cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    img = torch.Tensor(8, 3, 400, 600)\n",
    "    global_net = Global_pred()\n",
    "    gamma, color = global_net(img)\n",
    "    print(gamma.shape, color.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lin",
   "language": "python",
   "name": "lin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
